{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La limpieza de datos es esencial para eliminar o corregir valores faltantes, duplicados o incorrectos. La transformación de datos incluye la normalización, estandarización y aplicación de técnicas avanzadas de ingeniería de características, como las vistas en los capítulos anteriores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Cargar un conjunto de datos de ejemplo\n",
    "data = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Limpiar datos: eliminar duplicados y manejar valores faltantes\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "# Normalizar características numéricas\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data.select_dtypes(include=[np.number]))\n",
    "\n",
    "# Crear características polinómicas\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "data_poly = poly.fit_transform(data_scaled)\n",
    "\n",
    "# Agregar las características polinómicas al DataFrame original\n",
    "data_final = pd.DataFrame(data_poly, columns=poly.get_feature_names_out(data.columns))\n",
    "\n",
    "print(data_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La creación de nuevas características y la selección de las más relevantes es clave para optimizar el rendimiento del modelo. Usando técnicas avanzadas como operadores sobrecargados y funciones personalizadas, se pueden generar características que capturen relaciones complejas entre los datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear nuevas características utilizando operadores sobrecargados\n",
    "data_final['Ratio_Feature1_Feature2'] = data_final['feature1'] / data_final['feature2']\n",
    "data_final['Product_Feature3_Feature4'] = data_final['feature3'] * data_final['feature4']\n",
    "\n",
    "# Seleccionar las características más importantes utilizando un modelo de bosque aleatorio\n",
    "X = data_final.drop('target', axis=1)\n",
    "y = data_final['target']\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "importances = model.feature_importances_\n",
    "important_features = X.columns[importances > np.percentile(importances, 75)]\n",
    "\n",
    "print(f\"Características seleccionadas: {important_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las funciones personalizadas permiten transformar los datos de manera específica para el dominio del problema. Estas funciones pueden escalar, normalizar o transformar las características de acuerdo con las necesidades particulares del proyecto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir una función personalizada para escalar datos entre 0 y 1\n",
    "def escalar_personalizado(x, min_val, max_val):\n",
    "    return (x - min_val) / (max_val - min_val)\n",
    "\n",
    "# Aplicar la función personalizada a una característica específica\n",
    "data_final['Feature_Scaled'] = data_final['feature1'].apply(escalar_personalizado, args=(data_final['feature1'].min(), data_final['feature1'].max()))\n",
    "\n",
    "print(data_final['Feature_Scaled'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, el conjunto de datos procesado se utiliza para entrenar un modelo de machine learning. La evaluación del modelo se realiza utilizando métricas como la precisión, la sensibilidad o el error cuadrático medio, dependiendo del tipo de problema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[important_features], y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entrenar un modelo de Random Forest\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precisión del modelo: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp_coding_dojo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
