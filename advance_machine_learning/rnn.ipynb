{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Cargar el conjunto de datos IMDB\n",
    "max_features = 10000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# Padding de secuencias\n",
    "maxlen = 500\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "# Crear el modelo RNN\n",
    "model = Sequential([\n",
    "    Embedding(max_features, 32),\n",
    "    SimpleRNN(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 136ms/step - accuracy: 0.5697 - loss: 0.6652 - val_accuracy: 0.8120 - val_loss: 0.4375\n",
      "Epoch 2/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 158ms/step - accuracy: 0.7950 - loss: 0.4483 - val_accuracy: 0.7998 - val_loss: 0.4425\n",
      "Epoch 3/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 142ms/step - accuracy: 0.8997 - loss: 0.2575 - val_accuracy: 0.7970 - val_loss: 0.4445\n",
      "Epoch 4/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 134ms/step - accuracy: 0.8434 - loss: 0.3568 - val_accuracy: 0.7816 - val_loss: 0.4965\n",
      "Epoch 5/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 152ms/step - accuracy: 0.9641 - loss: 0.1174 - val_accuracy: 0.7908 - val_loss: 0.5540\n",
      "Epoch 6/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 183ms/step - accuracy: 0.9914 - loss: 0.0415 - val_accuracy: 0.8094 - val_loss: 0.6267\n",
      "Epoch 7/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 183ms/step - accuracy: 0.9986 - loss: 0.0147 - val_accuracy: 0.7716 - val_loss: 0.7436\n",
      "Epoch 8/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 180ms/step - accuracy: 0.9990 - loss: 0.0075 - val_accuracy: 0.7952 - val_loss: 0.7686\n",
      "Epoch 9/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 196ms/step - accuracy: 0.9896 - loss: 0.0324 - val_accuracy: 0.7774 - val_loss: 0.8134\n",
      "Epoch 10/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 145ms/step - accuracy: 0.9985 - loss: 0.0096 - val_accuracy: 0.7930 - val_loss: 0.8080\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 32ms/step - accuracy: 0.7874 - loss: 0.8312\n",
      "Test accuracy: 0.7891\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluar el modelo\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación del Código\n",
    "\n",
    "1. Carga del Conjunto de Datos: Usamos `imdb.load_data()` para cargar las reseñas de películas y sus etiquetas en conjuntos de entrenamiento y prueba.\n",
    "2. Padding de Secuencias: Las secuencias de texto se rellenan (padding) para que todas tengan la misma longitud, lo cual es necesario para procesarlas en una RNN.\n",
    "\n",
    "Explicación del Código\n",
    "\n",
    "1. Capa de Embedding: La capa de embedding convierte las palabras en vectores densos, lo que permite que la RNN procese secuencias de texto.\n",
    "2. Capa RNN: La capa `SimpleRNN` procesa las secuencias de texto y mantiene un estado oculto que se actualiza con cada palabra.\n",
    "3. Capa de Salida: La capa de salida utiliza una activación sigmoide para producir una probabilidad de clase, adecuada para la clasificación binaria.\n",
    "\n",
    "Explicación del Proceso de Entrenamiento\n",
    "\n",
    "1. Épocas: Entrenamos el modelo durante 10 épocas, donde cada época significa que el modelo verá todos los ejemplos de entrenamiento una vez.\n",
    "2. Batch Size: Usamos un tamaño de lote de 64, lo que significa que el modelo actualizará sus pesos después de ver 64 ejemplos.\n",
    "\n",
    "Interpretación de los Resultados\n",
    "\n",
    "1. Precisión del Modelo: La precisión en el conjunto de prueba nos da una idea de cuán bien el modelo generaliza a nuevas reseñas de películas.\n",
    "2. Pérdida en el Conjunto de Prueba: La pérdida en el conjunto de prueba es una métrica que indica qué tan bien el modelo está funcionando en términos de su función de pérdida.\n",
    "\n",
    "\n",
    "Mejoras y Ajustes\n",
    "\n",
    "Existen varias formas de mejorar y ajustar una RNN para obtener mejores resultados:\n",
    "\n",
    "1. Ajuste de Hiperparámetros: Puedes experimentar con diferentes tamaños de embedding, unidades de la RNN, y el tamaño del lote.\n",
    "2. Uso de LSTM o GRU: Considera el uso de LSTM o GRU en lugar de una SimpleRNN para manejar mejor dependencias a largo plazo.\n",
    "3. Aumento de Datos: Si es posible, puedes aumentar el conjunto de datos para mejorar la capacidad del modelo para generalizar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp_coding_dojo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
